# -*- coding: utf-8 -*-
"""BERT No Class-Weight MALAYALAM hasoc2021.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UB_Sjyt_WfpjjCe_P-fjhd8URaSlji1e

BERT Text Classification in a different language:
https://towardsdatascience.com/bert-text-classification-in-a-different-language-6af54930f9cb
"""

from google.colab import files
uploaded = files.upload()

# install simpletransformers
!pip install simpletransformers

# check installed version
!pip freeze | grep simpletransformers

import pandas as pd
from simpletransformers.classification import ClassificationModel

Mal_train=pd.read_csv("Mal_sentiment_full_train.tsv" , sep='\t')
Mal_Dev=pd.read_csv("Mal_sentiment_full_dev.tsv" , sep='\t')
Mal_test=pd.read_csv("Mal_sentiment_full_test_withoutlabels.tsv" , sep='\t')

pip install ml2en

from ml2en import ml2en

converter = ml2en()

desti_lang={
      'ml' : 'malayalam'
}

for key,value in desti_lang.items():
     #print(transt.translate(Mal_train['text']))
        Mal_train['text1']=Mal_train['text'].apply(lambda x: converter.transliterate(x))
        Mal_Dev['text1']=Mal_Dev['text'].apply(lambda x: converter.transliterate(x))
        Mal_test['text1']=Mal_test['text'].apply(lambda x: converter.transliterate(x))

Mal_train.head()

Mal_Dev.head()

Mal_test.head()

Mal_train['category'].value_counts()

Mal_Dev['category'].value_counts()

msg=Mal_train['text1']

msgDev=Mal_Dev['text1']

msgTest=Mal_test['text1']

import regex
import re
import numpy as np

#Remove Emojis

def emoji(msg): 
    
    msg_emoj=msg.str.replace(r'[(\U0001F600-\U0001F92F|\U0001F300-\U0001F5FF|\U0001F680-\U0001F6FF|\U0001F190-\U0001F1FF|\U00002702-\U000027B0|\U0001F926-\U0001FA9F|\u200d|\u2640-\u2642|\u2600-\u2B55|\u23cf|\u23e9|\u231a|\ufe0f)]+','')
    
    msg_digit=msg_emoj.str.replace(r'[0-9]', ' ')

    msg_Spac = msg_digit.str.replace(r'_',' ')

    msg_Spac = msg_Spac.str.replace(r'.','')

    msg_Spac = msg_Spac.str.replace(r'!','')

    msg_Spac = msg_Spac.str.replace(r'#','')

    msg_Spac = msg_Spac.str.replace(r'%','')

    msg_Spac = msg_Spac.str.replace(r'&','')

    msg_Spac = msg_Spac.str.replace(r'â€™','')

    msg_Spac = msg_Spac.str.replace(r'(','')

    msg_Spac = msg_Spac.str.replace(r')','')

    msg_Spac = msg_Spac.str.replace(r'-','')

    msg_Spac = msg_Spac.str.replace(r'/','')

    msg_Spac = msg_Spac.str.replace(r':','')

    msg_Spac = msg_Spac.str.replace(r';','')

    msg_Spac = msg_Spac.str.replace(r'<','')

    msg_Spac = msg_Spac.str.replace(r'=','')

    msg_Spac = msg_Spac.str.replace(r'>','')

    msg_Spac = msg_Spac.str.replace(r'?','')

    msg_Spac = msg_Spac.str.replace(r'@','')

    msg_Spac = msg_Spac.str.replace(r'[','')

    msg_Spac = msg_Spac.str.replace(r']','')

    msg_Spac = msg_Spac.str.replace(r'^','')

    msg_Spac = msg_Spac.str.replace(r'{','')

    msg_Spac = msg_Spac.str.replace(r'}','')

    msg_Spac = msg_Spac.str.replace(r'|','')

    msg_Spac = msg_Spac.str.replace(r'~','')

    msg_Spac = msg_Spac.str.replace(r'+','')

    msg_Spac = msg_Spac.str.replace(r'\s+', ' ')


    return msg_Spac

msg_Train=emoji(msg)

msg_Dev=emoji(msgDev)

msg_Test=emoji(msgTest)

Train_Df=pd.DataFrame(msg_Train)
Train_Df.columns=['Msg_with_Stopwords']

Dev_Df=pd.DataFrame(msg_Dev)
Dev_Df.columns=['Msg_with_Stopwords']

Test_Df=pd.DataFrame(msg_Test)
Test_Df.columns=['Msg_with_Stopwords']

Train_Df.shape, Dev_Df.shape, Test_Df.shape

# Removing Null Values

Train_Df['Msg_with_Stopwords']=Train_Df['Msg_with_Stopwords'].fillna("")

Dev_Df['Msg_with_Stopwords']=Dev_Df['Msg_with_Stopwords'].fillna("")

Test_Df['Msg_with_Stopwords']=Test_Df['Msg_with_Stopwords'].fillna("")

Mal_train.shape, Mal_Dev.shape,Train_Df.shape, Dev_Df.shape, Mal_test.shape, Test_Df.shape#

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

stop_words = set(stopwords.words('english'))

Train_Df['Msg_without_Stopwords']=Train_Df['Msg_with_Stopwords'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))

Dev_Df['Msg_without_Stopwords']=Dev_Df['Msg_with_Stopwords'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))

Test_Df['Msg_without_Stopwords']=Test_Df['Msg_with_Stopwords'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))

Train_Df['Msg_with_Stopwords_Len'] = [len(x.split()) for x in Train_Df['Msg_with_Stopwords'].tolist()]
Train_Df['Msg_without_Stopwords_Len'] = [len(x.split()) for x in Train_Df['Msg_without_Stopwords'].tolist()]

Dev_Df['Msg_with_Stopwords_Len'] = [len(x.split()) for x in Dev_Df['Msg_with_Stopwords'].tolist()]
Dev_Df['Msg_without_Stopwords_Len'] = [len(x.split()) for x in Dev_Df['Msg_without_Stopwords'].tolist()]

Test_Df['Msg_with_Stopwords_Len'] = [len(x.split()) for x in Test_Df['Msg_with_Stopwords'].tolist()]
Test_Df['Msg_without_Stopwords_Len'] = [len(x.split()) for x in Test_Df['Msg_without_Stopwords'].tolist()]

Train_Data=pd.concat([Mal_train, Train_Df], axis=1)

Dev_Data=pd.concat([Mal_Dev, Dev_Df], axis=1)

Test_Data=pd.concat([Mal_test, Test_Df], axis=1)

Train_Data.shape, Dev_Data.shape, Test_Data.shape

Train_Data=Train_Data.drop(['Msg_with_Stopwords','Msg_without_Stopwords_Len', 'Msg_with_Stopwords_Len', 'text'], axis=1)

Dev_Data=Dev_Data.drop(['Msg_with_Stopwords','Msg_without_Stopwords_Len', 'Msg_with_Stopwords_Len', 'text'], axis=1)

Test_Data=Test_Data.drop(['Msg_with_Stopwords','Msg_without_Stopwords_Len', 'Msg_with_Stopwords_Len', 'text'], axis=1)

Train_Data.shape, Dev_Data.shape, Test_Data.shape

Train_Data.head()

from sklearn import preprocessing
le = preprocessing.LabelEncoder()

Train_Data['Nlabel']=le.fit_transform(Train_Data['category']) 
Dev_Data['Nlabel']=le.fit_transform(Dev_Data['category'])

Train_Data.head()

Train_Data=Train_Data.drop(['text1','category'], axis=1)

Dev_Data=Dev_Data.drop(['text1','category'], axis=1)

Test_Data=Test_Data.drop(['text1'], axis=1)

Train_Data.head()

Dev_Data.head()

Test_Data.head(2)

#Train_Data=Train_Data.drop(['text1','Mixed_feelings', 'Negative', 'Positive', 'not-malayalam', 'unknown_state'], axis=1)
#Train_Data

#from sklearn.model_selection import train_test_split

#train_df, test_df = train_test_split(Train_Data, test_size=0.20)

#print('train shape: ',train_df.shape)
#print('test shape: ',test_df.shape)

from sklearn.utils import class_weight

#class_weights = class_weight.compute_class_weight('balanced',
 #                                                np.unique(Train_Data.Nlabel),
  #                                               Train_Data.Nlabel)
#class_weights

# define hyperparameter
model_args ={"reprocess_input_data": True,
             "fp16":False,
             "num_train_epochs": 4,
             "overwrite_output_dir" : True}

# Create a ClassificationModel
model = ClassificationModel(
    "bert", "bert-base-cased",
    num_labels=5,
   # pos_weight=[3.43153348, 1.50954869, 0.49487619, 2.74641314, 0.60193218],
    args=model_args
)

model.train_model(Train_Data)

from sklearn.metrics import f1_score, accuracy_score


def f1_multiclass(labels, preds):
    return f1_score(labels, preds, average='micro')
    
result, model_outputs, wrong_predictions = model.eval_model(Dev_Data, f1=f1_multiclass, acc=accuracy_score)

result

prediction=model.predict(Dev_Data['Msg_without_Stopwords'].values.tolist())

prediction=prediction[0]
prediction

type(prediction), prediction.shape

"""bert-base-cased
{'acc': 0.7321630804077011,
 'eval_loss': 1.2600493266882493,
 'f1': 0.7321630804077011,
 'mcc': 0.6160345646130457}

 precision    recall  f1-score   support

           0       0.50      0.38      0.43       102
           1       0.61      0.57      0.59       237
           2       0.76      0.82      0.79       706
           3       0.87      0.84      0.85       141
           4       0.74      0.72      0.73       580

    accuracy                           0.73      1766
   macro avg       0.70      0.67      0.68      1766
weighted avg       0.73      0.73      0.73      1766

**bert-base-uncased** {'acc': 0.6938325991189427,
 'eval_loss': 1.1257254117257407,
 'f1': 0.6938325991189427,
 'mcc': 0.556442235554547}

bert-base-multilingual-cased** : {{'acc': 0.7485843714609286,
 'eval_loss': 1.0802257422092307,
 'f1': 0.7485843714609286,
 'mcc': 0.6417533819954006}

  precision    recall  f1-score   support

           0       0.44      0.39      0.42       102
           1       0.68      0.61      0.64       237
           2       0.79      0.79      0.79       706
           3       0.85      0.84      0.84       141
           4       0.73      0.78      0.75       580

    accuracy                           0.74      1766
   macro avg       0.70      0.68      0.69      1766
weighted avg       0.74      0.74      0.74      1766
"""

tp - True positives 257
tn - True negatives 297
fp - False positives 99
fn - False negatives 147

Precision = TruePositives / (TruePositives + FalsePositives)
Precision = 45 / (45 + 5)

Recall = TruePositives / (TruePositives + FalseNegatives)
Recall = 95 / (95 + 5)
Recall = 0.95

F-Measure = (2 * Precision * Recall) / (Precision + Recall)
F-Measure = (2 * 0.633 * 0.95) / (0.633 + 0.95)
F-Measure = (2 * 0.601) / 1.583
F-Measure = 1.202 / 1.583
F-Measure = 0.759

import numpy as np
from sklearn.metrics import classification_report
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import precision_score, recall_score, f1_score

X_dev1=Dev_Data['Msg_without_Stopwords']
y_dev1=Dev_Data['Nlabel']

print(classification_report(y_dev1, prediction))

precision = precision_score(y_dev1, prediction, average='weighted')
print('Precision: %f' % precision)
recall = recall_score(y_dev1, prediction, average='weighted')
print('Recall: %f' % recall)
f1 = f1_score(y_dev1, prediction, average='weighted')
print('F1 score: %f' % f1)

def predict(X_dev1,y_dev1,X_test1):
    loss, accuracy = model.evaluate(X_dev1,y_dev1,verbose=0)
    print('Accuracy: %f' % (accuracy*100))
    print('Loss: %f' % (loss*100))
    y_pred=model.predict_classes(X_dev1)

    # predict 
    predictions = model.predict(X_dev1, batch_size = 32)
    pred = np.argmax(predictions, axis=1)
    # label
    y_dev1 = np.argmax(y_dev1, axis=1)
    
    print(classification_report(y_dev1, pred))
    
    cohen_score = cohen_kappa_score(y_dev1, pred)
    print("Cohen Score: ",cohen_score)
    ##### PREDICTING test values ######
    y_pred=model.predict_classes(X_test1)
    df = pd.DataFrame(y_pred, columns = ['result'])
    return df

def predict(X_dev1,y_dev1,X_test1):
    loss, accuracy = model.evaluate(X_dev1,y_dev1,verbose=0)
    print('Accuracy: %f' % (accuracy*100))
    print('Loss: %f' % (loss*100))
    y_pred=model.predict_classes(X_dev1)

    # predict 
    predictions = model.predict(X_dev1, batch_size = 32)
    pred = np.argmax(predictions, axis=1)
    # label
    y_dev1 = np.argmax(y_dev1, axis=1)
    
    print(classification_report(y_dev1, pred))
    
    cohen_score = cohen_kappa_score(y_dev1, pred)
    print("Cohen Score: ",cohen_score)
    ##### PREDICTING test values ######
    y_pred=model.predict_classes(X_test1)
    df = pd.DataFrame(y_pred, columns = ['result'])
    return df